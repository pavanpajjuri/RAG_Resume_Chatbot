{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ayy41aLeRF_",
        "outputId": "a45c5dbe-52a1-4c66-e214-4d7eaaa93a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.5.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.1)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.72-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.5.2-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.72-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=56f974f7782996c9e96a4136e5417e4e864ddb1caa68ffbb8b3189e70bc9bb44\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, python-dotenv, pypdf, pybase64, overrides, ormsgpack, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, bcrypt, backoff, watchfiles, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, onnxruntime, langgraph-sdk, kubernetes, dataclasses-json, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, langgraph-checkpoint, langchain_openai, langgraph-prebuilt, chromadb, langgraph, langchain_community\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 httptools-0.6.4 httpx-sse-0.4.1 humanfriendly-10.0 kubernetes-33.1.0 langchain_community-0.3.27 langchain_openai-0.3.27 langgraph-0.5.2 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.72 marshmallow-3.26.1 mmh3-5.1.0 mypy-extensions-1.1.0 onnxruntime-1.22.1 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 ormsgpack-1.10.0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.1 pydantic-settings-2.10.1 pypdf-5.7.0 pypika-0.48.9 python-dotenv-1.1.1 typing-inspect-0.9.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community chromadb openai tiktoken langchain_openai langgraph pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Langchain and Langsmith APIs\n",
        "# Langchain: A framework to build LLM-powered applications by composing prompts, models, tools, memory, and chains.\n",
        "# Langsmith: A debugging, observability, and evaluation platform for LangChain workflows. Helps inspect inputs, outputs, traces, and improve LLM app quality.\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4Qd65uVeZBV",
        "outputId": "355b757d-a79c-4167-8bb4-05a54c047732"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI API\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
      ],
      "metadata": {
        "id": "Mf5Fu7h_ec51"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Using GPT 3.5 for chat llm and text-embedding-3-large for embeddings\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "# llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
        "\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model = 'text-embedding-3-large')"
      ],
      "metadata": {
        "id": "nbKIRfuEejig"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "resume_filenames = [\"Resume_Pavan.pdf\", \"Resume_Pavan_DS.pdf\"]\n",
        "all_docs = []\n",
        "for filename in resume_filenames:\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"- Loading {filename}...\")\n",
        "        loader = PyPDFLoader(filename)\n",
        "        docs_from_pdf = loader.load()\n",
        "\n",
        "        # 3. Add the source filename to each page's metadata. This is important!\n",
        "        for doc in docs_from_pdf:\n",
        "            doc.metadata['source'] = filename\n",
        "\n",
        "        all_docs.extend(docs_from_pdf)\n",
        "    else:\n",
        "        # If a file is not found, print a warning and continue.\n",
        "        print(f\"- WARNING: The file '{filename}' was not found. Skipping.\")\n",
        "\n",
        "print(\"\\n---------------------------------\")\n",
        "print(\"Resume loading complete.\")\n",
        "print(f\"Total pages loaded: {len(all_docs)}\")\n",
        "print(f\"Total characters: {sum(len(doc.page_content) for doc in all_docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcKqmvSTu1rP",
        "outputId": "6b3377cb-81dd-4c8f-f75b-8a060cb1c581"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Loading Resume_Pavan.pdf...\n",
            "- Loading Resume_Pavan_DS.pdf...\n",
            "\n",
            "---------------------------------\n",
            "Resume loading complete.\n",
            "Total pages loaded: 2\n",
            "Total characters: 9400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# # Initialize a recursive text splitter to divide the document into chunks of max 500 characters,\n",
        "# # with a 200-character overlap between consecutive chunks. This overlap helps preserve context continuity.\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size = 1000,\n",
        "#     chunk_overlap = 200,\n",
        "#     separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
        "#     add_start_index = True,\n",
        "# )\n",
        "# all_splits = text_splitter.split_documents(all_docs)\n",
        "\n",
        "# print(f\"Split blog post into {len(all_splits)} sub-documents\")"
      ],
      "metadata": {
        "id": "osm6B2BOfR7X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Splitting on sections and then recursive splitting\n",
        "\n",
        "import re\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def split_by_section(text):\n",
        "    # Define major section headers\n",
        "    pattern = r\"(PROFESSIONAL SUMMARY|EXPERIENCE|PROJECTS|EDUCATION|TECHNICAL SKILLS)\"\n",
        "    splits = re.split(pattern, text)\n",
        "\n",
        "    # Reconstruct pairs of (header, content)\n",
        "    chunks = []\n",
        "    for i in range(1, len(splits), 2):\n",
        "        section = splits[i].strip()\n",
        "        content = splits[i+1].strip()\n",
        "        full_content = f\"{section}\\n{content}\"\n",
        "        chunks.append(Document(page_content=full_content, metadata={\"section\": section.lower()}))\n",
        "    return chunks\n",
        "\n",
        "section_splits = []\n",
        "for doc in all_docs:\n",
        "    if isinstance(doc, Document):\n",
        "        text = doc.page_content\n",
        "    else:\n",
        "        text = str(doc)  # fallback if plain string\n",
        "    section_splits.extend(split_by_section(text))\n",
        "\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# Initialize a recursive text splitter to divide the document into chunks of max 1000 characters,\n",
        "# with a 200-character overlap between consecutive chunks. This overlap helps preserve context continuity.\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
        "    add_start_index=True\n",
        ")\n",
        "\n",
        "all_splits = text_splitter.split_documents(section_splits)\n",
        "\n",
        "print(f\"Split resumes into {len(all_splits)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_pxpP2FT6nz",
        "outputId": "335f800a-4bf4-4c10-9461-a048923ad37a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split resumes into 16 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for split in all_splits:\n",
        "    print(split.page_content)\n",
        "    print(\"---------------------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J39z_-hzQnDX",
        "outputId": "fe41d3d6-bef9-4c1f-adea-0458b02fc2c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROFESSIONAL SUMMARY\n",
            "Machine Learning Engineer with 3+ years of experience delivering AI/ML solutions across recommendation systems, generative\n",
            "modeling, and NLP. Proven ability to translate advanced research on LLMs, GANs into scalable, interpretable systems that drive\n",
            "measurable business impact. Skilled at bridging experimentation and deployment in high-stakes, production-grade environments.\n",
            "---------------------------------------\n",
            "\n",
            "EXPERIENCE\n",
            "PayPal India | Machine Learning Engineer Jan 2021 - Jan 2024\n",
            "Python, SQL, Machine Learning, CNN, NLP, Tableau Hyderabad, India\n",
            "• Boosted revenue by onboarding 50+ global merchants preventing $2M in fraud losses through ML-based fraud detection\n",
            "models. Designed decision tree and anomaly detection techniques to analyze transactions and identify fraudulent patterns\n",
            "• Built a Siamese CNN model with custom contrastive loss to detect document’s manipulation by comparing against true\n",
            "embeddings. Achieved 94% precision, reducing manual review by 45%, preventing onboarding of $1.2M in high-risk accounts\n",
            "• Fine-tuned a domain-specific LLaMA-2 model on 100K+ fraud analyst notes using NLP techniques to generate soft-approval\n",
            "summaries for flagged chargeback transactions, reducing triage time by 32%\n",
            "• Reproduced Google’s TabNet architecture and integrated SHAP/LIME for feature attribution and model explainability on\n",
            "---------------------------------------\n",
            "\n",
            "summaries for flagged chargeback transactions, reducing triage time by 32%\n",
            "• Reproduced Google’s TabNet architecture and integrated SHAP/LIME for feature attribution and model explainability on\n",
            "250K+ transaction dataset; improved fraud rejection precision from 55% to 67%\n",
            "• Automated Interactive KPI dashboards using Tableau for real-time fraud tracking, increasing merchant retention by 70%\n",
            "• Partnered with product managers to design A/B tests. Applied causal inference and uplift modeling to evaluate fraud detection\n",
            "rules, leading to a 12% increase in accuracy without increasing false positives, while ensuring statistical significance\n",
            "• Built automated feature pipelines using SQL and PySpark on Cassandra DB, enabling dynamic feature generation and reducing\n",
            "query latency by 40%, while collaborating with product and engineering teams to align on fraud strategy\n",
            "Samsung R & D Institute | Machine Learning Intern May 2020 - July 2020\n",
            "---------------------------------------\n",
            "\n",
            "query latency by 40%, while collaborating with product and engineering teams to align on fraud strategy\n",
            "Samsung R & D Institute | Machine Learning Intern May 2020 - July 2020\n",
            "Tensorflow, Deep Learning, Knowledge Graphs, Recommendation Systems Noida, India\n",
            "• Collaborated with Senior Manager to optimize recommender systems by embedding visual features and CNNs with Knowledge\n",
            "Graphs, boosting recommendation accuracy by 30% and user engagement by 20%\n",
            "• Optimized path-based KPRN recommendation model in Keras and evaluated on enriched MovieLens 1M dataset against\n",
            "benchmarked baseline classifiers for Top-K recommendations. Achieved Precision@10 (Top 10) with a 0.086 score\n",
            "• Integrated algorithm into Samsung’s chatbot, refining response relevance by 20% for 100K+ users\n",
            "---------------------------------------\n",
            "\n",
            "PROJECTS\n",
            "End-to-End Deployment of a Text-to-Image GAN | Github\n",
            "Python, Pytorch, GANs, NLP, AWS\n",
            "• Objective: Develop high-fidelity Text-to-Image GAN capable of producing realistic and diverse images from textual descriptions\n",
            "• Designed the model in PyTorch from scratch, attaining anInception Score of 4.5, FID of 147.4, and CLIP Score of 0.56, with a\n",
            "20% improvement over baseline models by training on CUB (Birds) dataset\n",
            "• Architected an end-to-end MLOps pipeline on AWS SageMaker to automate model training, deployment, and inference,lowering\n",
            "process time by 40% and cutting inference latency by 35%, ensuring seamless real-time image generation with scalable API\n",
            "Fine-Tuning LLM for Conversational AI| Github\n",
            "Python, PyTorch, Hugging Face Transformers, AWS\n",
            "• Objective: Fine-tune a large language model (flan-T5) for dialogue summarization and contextual relevance\n",
            "• Incorporated Low-Rank Adaptation (LoRA) to minimize computational overhead and enhance model efficiency . Applied\n",
            "---------------------------------------\n",
            "\n",
            "• Incorporated Low-Rank Adaptation (LoRA) to minimize computational overhead and enhance model efficiency . Applied\n",
            "Reinforcement Learning (PPO) with reward modeling to improve policy optimization and LLM behavior\n",
            "• Reduced toxicity by 40%, improved BLEU/ROUGE by 30%, and outperformed zero-shot GPT baseline by 22%, while cutting\n",
            "trainable parameters and memory footprint by 50%, ensuring efficient and reliable generation\n",
            "---------------------------------------\n",
            "\n",
            "EDUCATION\n",
            "University at Buffalo SUNY | Buffalo, NY Jan 2024 - May 2025\n",
            "Master of Science in Data Science GPA : 4.0\n",
            "Indian Institute of Technology Madras (IITM) | Chennai, India July 2016 - May 2021\n",
            "Bachelor and Master of Technology in Electrical Engineering\n",
            "---------------------------------------\n",
            "\n",
            "TECHNICAL SKILLS\n",
            "Languages : Python, PySpark, SQL (Postgres), R, C/C++\n",
            "Machine Learning & AI : PyTorch, TensorFlow, OpenCV, Scikit-learn, MLOps, CNNs, NLP, Transformers, LLMs, RAG, LangChain\n",
            "Statistical Modeling : Regression, Time Series, Bayesian Inference, Monte Carlo Methods, Optimization\n",
            "Big Data & Cloud : AWS (SageMaker, Lambda, S3), PySpark, Hadoop\n",
            "Tools & Software : Tableau, Power BI, Streamlit, Excel, Git, MATLAB, Visual Studio\n",
            "---------------------------------------\n",
            "\n",
            "PROFESSIONAL SUMMARY\n",
            "Data Scientist with 3+ years of experience developing predictive models, A/B tests, and causal inference frameworks across fraud\n",
            "detection, recommendation systems, and customer analytics. Skilled at translating complex data into actionable insights through\n",
            "statistical modeling, uplift modeling, and real-time dashboards. Experienced in Python, SQL, Tableau, and PySpark on AWS-scale\n",
            "systems, with a track record of cross-functional collaboration and data-driven decision making.\n",
            "---------------------------------------\n",
            "\n",
            "EXPERIENCE\n",
            "PayPal India | Data Scientist Jan 2021 - Jan 2024\n",
            "Python, SQL, Machine Learning, NLP, Tableau Hyderabad, India\n",
            "• Boosted revenue by onboarding 50+ global merchants preventing $2M in fraud losses through ML-based fraud detection\n",
            "models. Designed decision tree and anomaly detection techniques to analyze transactions and identify fraudulent patterns\n",
            "• Leveraged NLP to generate chargeback approval summaries from 100K+ fraud analyst notes, reducing triage time by 32% and\n",
            "accelerating manual review decisions\n",
            "• Partnered with product managers to design A/B tests. Applied causal inference and uplift modeling, improving fraud rule\n",
            "accuracy by 12% without increasing false positives, while ensuring statistical significance\n",
            "• Built time-series anomaly detection (Prophet) models to detect temporal spikes, enhancing early fraud detection by 18%\n",
            "• Automated real-time KPI dashboards in Tableau, driving 70% increase in merchant retention by enabling faster fraud triage\n",
            "---------------------------------------\n",
            "\n",
            "• Automated real-time KPI dashboards in Tableau, driving 70% increase in merchant retention by enabling faster fraud triage\n",
            "• Enhanced model explainability and feature importance by implementing Google’s TabNet with LIME & SHAP on a 250K+\n",
            "transaction dataset, increasing fraud rejection precision from 55% to 67%\n",
            "• Developed dynamic feature generation pipelines using SQL and PySpark on Cassandra, reducing query latency by 40% in\n",
            "collaboration with product and engineering teams to align features with evolving fraud strategies\n",
            "Samsung R & D Institute | Machine Learning Intern May 2020 - July 2020\n",
            "Tensorflow, Deep Learning, NLP, Knowledge Graphs, Recommendation Systems Noida, India\n",
            "• Collaborated with Senior Manager to optimize recommender systems by embedding visual features and CNNs with Knowledge\n",
            "Graphs, boosting recommendation accuracy by 30% and user engagement by 20%\n",
            "• Optimized path-based KPRN recommendation model in Keras and evaluated on enriched MovieLens 1M dataset against\n",
            "---------------------------------------\n",
            "\n",
            "Graphs, boosting recommendation accuracy by 30% and user engagement by 20%\n",
            "• Optimized path-based KPRN recommendation model in Keras and evaluated on enriched MovieLens 1M dataset against\n",
            "benchmarked baseline classifiers for Top-K recommendations. Achieved Precision@10 (Top 10) with a 0.086 score\n",
            "• Integrated algorithm into Samsung’s chatbot, refining response relevance by 20% for 100K+ users\n",
            "---------------------------------------\n",
            "\n",
            "PROJECTS\n",
            "End-to-End Deployment of a Text-to-Image GAN | Github\n",
            "Python, Pytorch, GANs, NLP, AWS\n",
            "• Objective: Develop high-fidelity Text-to-Image GAN capable of producing realistic and diverse images from textual descriptions\n",
            "• Designed the model in PyTorch from scratch, attaining anInception Score of 4.5, FID of 147.4, and CLIP Score of 0.56, with a\n",
            "20% improvement over baseline models by training on CUB (Birds) dataset\n",
            "• Architected an end-to-end MLOps pipeline on AWS SageMaker to automate model training, deployment, and inference,lowering\n",
            "process time by 40% and cutting inference latency by 35%, ensuring seamless real-time image generation with scalable API\n",
            "Fine-Tuning LLM for Conversational AI| Github\n",
            "Python, PyTorch, Hugging Face Transformers, AWS\n",
            "• Objective: Fine-tune a large language model (flan-T5) for dialogue summarization and contextual relevance\n",
            "• Incorporated Low-Rank Adaptation (LoRA) to minimize computational overhead and enhance model efficiency . Applied\n",
            "---------------------------------------\n",
            "\n",
            "• Incorporated Low-Rank Adaptation (LoRA) to minimize computational overhead and enhance model efficiency . Applied\n",
            "Reinforcement Learning (PPO) with reward modeling to improve policy optimization and LLM behavior\n",
            "• Reduced toxicity by 40%, improved BLEU/ROUGE by 30%, and outperformed zero-shot GPT baseline by 22%, while cutting\n",
            "trainable parameters and memory footprint by 50%, ensuring efficient and reliable generation\n",
            "---------------------------------------\n",
            "\n",
            "EDUCATION\n",
            "University at Buffalo SUNY | Buffalo, NY Jan 2024 - May 2025\n",
            "Master of Science in Data Science GPA : 4.0\n",
            "Indian Institute of Technology Madras (IITM) | Chennai, India July 2016 - May 2021\n",
            "Bachelor and Master of Technology in Electrical Engineering\n",
            "---------------------------------------\n",
            "\n",
            "TECHNICAL SKILLS\n",
            "Languages : Python, SQL (Postgres), PySpark, R, C/C++\n",
            "Machine Learning & AI : PyTorch, TensorFlow, OpenCV, Scikit-learn, CNNs, NLP, Transformers, LLMs, RAG, LangChain\n",
            "Statistical Modeling : Regression, Time Series, Bayesian Inference, Monte Carlo Methods, Optimization\n",
            "Big Data & Cloud : AWS (SageMaker, Lambda, S3), PySpark, Hadoop\n",
            "Tools & Software : Tableau, Power BI, Streamlit, Excel, Git, MATLAB, Visual Studio\n",
            "---------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "vector_store = Chroma.from_documents(documents=all_splits,\n",
        "                                    embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "wOBcJHjvfZRg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState, StateGraph\n",
        "graph_builder = StateGraph(MessagesState)"
      ],
      "metadata": {
        "id": "X5TG7s7IfqTz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "# Define a tool named 'retrieve' using the @tool decorator.\n",
        "# This tool will return both a formatted string and the actual documents as output.\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve(query: str):\n",
        "    \"\"\"Retrieve information related to a query.\"\"\"\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=7)\n",
        "    serialized = \"\\n\\n\".join(\n",
        "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    # Return both the serialized string (for LLM consumption) and the raw documents (for structured use).\n",
        "    return serialized, retrieved_docs"
      ],
      "metadata": {
        "id": "dBu46-JxfsPE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "\n",
        "def query_or_respond(state: MessagesState):\n",
        "  # Bind the 'retrieve' tool to the LLM for dynamic tool-calling.\n",
        "  llm_with_tools = llm.bind_tools([retrieve])\n",
        "  # Invoke the LLM with the current conversation messages (state[\"messages\"]).\n",
        "  # The LLM will decide whether to call the tool or return a direct answer.\n",
        "  response = llm_with_tools.invoke(state['messages'])\n",
        "  return {\"messages\":[response]}\n",
        "\n",
        "\n",
        "# Define the tool execution node.\n",
        "# ToolNode handles execution of tool calls that were requested by the LLM in the previous step.\n",
        "# It automatically routes tool-calls to the correct function (e.g., `retrieve`) and returns the tool output as a ToolMessage.\n",
        "\n",
        "tools = ToolNode([retrieve])\n",
        "\n",
        "\n",
        "#\t•\tquery_or_respond() = Decision node: LLM decides whether to use a tool or not.\n",
        "#\t•\tToolNode([retrieve]) = Execution node: Actually runs the tool when called.\n",
        "\n",
        "def generate(state: MessagesState):\n",
        "  # Extract the most recent tool messages in reverse order (latest first)\n",
        "  recent_tool_messages = []\n",
        "  for message in reversed(state[\"messages\"]):\n",
        "    if message.type == \"tool\":\n",
        "      recent_tool_messages.append(message)\n",
        "    else:\n",
        "      break # Stop when we hit a non-tool message to isolate the latest tool-call output\n",
        "\n",
        "  # Reverse the order to restore chronological flow (oldest to latest)\n",
        "  tool_messages = recent_tool_messages[::-1]\n",
        "\n",
        "  docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
        "\n",
        "  # This is our new, much more detailed system prompt.\n",
        "  system_message_content = (\n",
        "    \"You are a professional HR assistant. Your role is to answer questions strictly based on the candidate's resume information provided below.\\n\\n\"\n",
        "    \"The context block may contain excerpts from one or more resumes. Please follow these instructions:\\n\"\n",
        "    \"1. Base your answer only on the provided '---CONTEXT---' block. You may use any retrieved context to answer, even if not explicitly labeled.\"\n",
        "    \"2. Scan all chunks in the context, especially sections like EDUCATION, EXPERIENCE, and PROJECTS, before answering.\"\n",
        "    \"3. If the source is specified (e.g., 'Source Page: ...'), use it to determine which candidate's resume the content belongs to.\\n\"\n",
        "    \"4. If the information needed is not found in the context, respond with: 'This information is not available in the provided resume.' Do not guess or fabricate answers.\\n\"\n",
        "    \"5. Keep your response concise, formal, and relevant to the query.\\n\\n\"\n",
        "    \"---CONTEXT---\\n\"\n",
        "    f\"{docs_content}\"\n",
        "    \"\\n---END CONTEXT---\"\n",
        ")\n",
        "\n",
        "  conversation_messages = []\n",
        "  for message in state[\"messages\"]:\n",
        "    if message.type in (\"human\",\"system\") or (message.type == \"ai\" and not message.tool_calls):\n",
        "      conversation_messages.append(message)\n",
        "\n",
        "  # Prepend the system instruction and retrieved context to the conversation\n",
        "  prompt = [SystemMessage(content = system_message_content)] + conversation_messages\n",
        "  response = llm.invoke(prompt)\n",
        "  return {\"messages\":[response]}"
      ],
      "metadata": {
        "id": "YecARmabfv2v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# Add a node that decides whether to respond directly or make a tool call\n",
        "graph_builder.add_node(\"query_or_respond\", query_or_respond)\n",
        "# Add a predefined tool execution node to handle any tool calls (e.g., retrieval)\n",
        "graph_builder.add_node(\"tools\", tools)\n",
        "# Add a node that generates the final answer using retrieved content (from tool messages)\n",
        "graph_builder.add_node(\"generate\", generate)\n",
        "\n",
        "graph_builder.set_entry_point(\"query_or_respond\")\n",
        "# If the LLM response includes a tool call, route to \"tools\"; otherwise, end the graph.\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"query_or_respond\",     # Source node\n",
        "    tools_condition,        # Function that inspects response for tool calls\n",
        "    {END: END, \"tools\": \"tools\"}  # Destination mapping based on condition\n",
        ")\n",
        "\n",
        "# Define that after tool execution, go to the generate node\n",
        "graph_builder.add_edge(\"tools\", \"generate\")\n",
        "graph_builder.add_edge(\"generate\", END)\n",
        "\n",
        "\n",
        "# Initialize an in-memory checkpointer to persist chat state between steps\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "# Specify an ID for the thread\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
      ],
      "metadata": {
        "id": "avxp0Rdkg4Vu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_message = \"Give summary about Pavan?\"\n",
        "\n",
        "for step in graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
        "    stream_mode=\"values\",\n",
        "    config=config,\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNbOD72ihAru",
        "outputId": "d9262599-5c1d-4696-88ec-2ced24fb7956"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Give summary about Pavan?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve (call_6QhjntrK47dsHfWbslyjGgXk)\n",
            " Call ID: call_6QhjntrK47dsHfWbslyjGgXk\n",
            "  Args:\n",
            "    query: Summary about Pavan\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: retrieve\n",
            "\n",
            "Source: {'start_index': 0, 'section': 'professional summary'}\n",
            "Content: PROFESSIONAL SUMMARY\n",
            "Data Scientist with 3+ years of experience developing predictive models, A/B tests, and causal inference frameworks across fraud\n",
            "detection, recommendation systems, and customer analytics. Skilled at translating complex data into actionable insights through\n",
            "statistical modeling, uplift modeling, and real-time dashboards. Experienced in Python, SQL, Tableau, and PySpark on AWS-scale\n",
            "systems, with a track record of cross-functional collaboration and data-driven decision making.\n",
            "\n",
            "Source: {'start_index': 0, 'section': 'professional summary'}\n",
            "Content: PROFESSIONAL SUMMARY\n",
            "Machine Learning Engineer with 3+ years of experience delivering AI/ML solutions across recommendation systems, generative\n",
            "modeling, and NLP. Proven ability to translate advanced research on LLMs, GANs into scalable, interpretable systems that drive\n",
            "measurable business impact. Skilled at bridging experimentation and deployment in high-stakes, production-grade environments.\n",
            "\n",
            "Source: {'start_index': 0, 'section': 'experience'}\n",
            "Content: EXPERIENCE\n",
            "PayPal India | Data Scientist Jan 2021 - Jan 2024\n",
            "Python, SQL, Machine Learning, NLP, Tableau Hyderabad, India\n",
            "• Boosted revenue by onboarding 50+ global merchants preventing $2M in fraud losses through ML-based fraud detection\n",
            "models. Designed decision tree and anomaly detection techniques to analyze transactions and identify fraudulent patterns\n",
            "• Leveraged NLP to generate chargeback approval summaries from 100K+ fraud analyst notes, reducing triage time by 32% and\n",
            "accelerating manual review decisions\n",
            "• Partnered with product managers to design A/B tests. Applied causal inference and uplift modeling, improving fraud rule\n",
            "accuracy by 12% without increasing false positives, while ensuring statistical significance\n",
            "• Built time-series anomaly detection (Prophet) models to detect temporal spikes, enhancing early fraud detection by 18%\n",
            "• Automated real-time KPI dashboards in Tableau, driving 70% increase in merchant retention by enabling faster fraud triage\n",
            "\n",
            "Source: {'section': 'experience', 'start_index': 0}\n",
            "Content: EXPERIENCE\n",
            "PayPal India | Machine Learning Engineer Jan 2021 - Jan 2024\n",
            "Python, SQL, Machine Learning, CNN, NLP, Tableau Hyderabad, India\n",
            "• Boosted revenue by onboarding 50+ global merchants preventing $2M in fraud losses through ML-based fraud detection\n",
            "models. Designed decision tree and anomaly detection techniques to analyze transactions and identify fraudulent patterns\n",
            "• Built a Siamese CNN model with custom contrastive loss to detect document’s manipulation by comparing against true\n",
            "embeddings. Achieved 94% precision, reducing manual review by 45%, preventing onboarding of $1.2M in high-risk accounts\n",
            "• Fine-tuned a domain-specific LLaMA-2 model on 100K+ fraud analyst notes using NLP techniques to generate soft-approval\n",
            "summaries for flagged chargeback transactions, reducing triage time by 32%\n",
            "• Reproduced Google’s TabNet architecture and integrated SHAP/LIME for feature attribution and model explainability on\n",
            "\n",
            "Source: {'section': 'experience', 'start_index': 854}\n",
            "Content: • Automated real-time KPI dashboards in Tableau, driving 70% increase in merchant retention by enabling faster fraud triage\n",
            "• Enhanced model explainability and feature importance by implementing Google’s TabNet with LIME & SHAP on a 250K+\n",
            "transaction dataset, increasing fraud rejection precision from 55% to 67%\n",
            "• Developed dynamic feature generation pipelines using SQL and PySpark on Cassandra, reducing query latency by 40% in\n",
            "collaboration with product and engineering teams to align features with evolving fraud strategies\n",
            "Samsung R & D Institute | Machine Learning Intern May 2020 - July 2020\n",
            "Tensorflow, Deep Learning, NLP, Knowledge Graphs, Recommendation Systems Noida, India\n",
            "• Collaborated with Senior Manager to optimize recommender systems by embedding visual features and CNNs with Knowledge\n",
            "Graphs, boosting recommendation accuracy by 30% and user engagement by 20%\n",
            "• Optimized path-based KPRN recommendation model in Keras and evaluated on enriched MovieLens 1M dataset against\n",
            "\n",
            "Source: {'start_index': 0, 'section': 'technical skills'}\n",
            "Content: TECHNICAL SKILLS\n",
            "Languages : Python, PySpark, SQL (Postgres), R, C/C++\n",
            "Machine Learning & AI : PyTorch, TensorFlow, OpenCV, Scikit-learn, MLOps, CNNs, NLP, Transformers, LLMs, RAG, LangChain\n",
            "Statistical Modeling : Regression, Time Series, Bayesian Inference, Monte Carlo Methods, Optimization\n",
            "Big Data & Cloud : AWS (SageMaker, Lambda, S3), PySpark, Hadoop\n",
            "Tools & Software : Tableau, Power BI, Streamlit, Excel, Git, MATLAB, Visual Studio\n",
            "\n",
            "Source: {'start_index': 0, 'section': 'technical skills'}\n",
            "Content: TECHNICAL SKILLS\n",
            "Languages : Python, SQL (Postgres), PySpark, R, C/C++\n",
            "Machine Learning & AI : PyTorch, TensorFlow, OpenCV, Scikit-learn, CNNs, NLP, Transformers, LLMs, RAG, LangChain\n",
            "Statistical Modeling : Regression, Time Series, Bayesian Inference, Monte Carlo Methods, Optimization\n",
            "Big Data & Cloud : AWS (SageMaker, Lambda, S3), PySpark, Hadoop\n",
            "Tools & Software : Tableau, Power BI, Streamlit, Excel, Git, MATLAB, Visual Studio\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Pavan is a skilled professional with 3+ years of experience in Data Science and Machine Learning Engineering. He has a proven track record of developing predictive models, A/B tests, and causal inference frameworks across various domains such as fraud detection, recommendation systems, and customer analytics. Pavan is proficient in Python, SQL, Tableau, and PySpark, with experience working on AWS-scale systems. He has a strong background in statistical modeling, uplift modeling, and real-time dashboard creation. Pavan has successfully collaborated cross-functionally and made data-driven decisions to drive measurable business impact.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "print(\"Resume Q&A Chatbot (type 'exit' to quit)\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    result = graph.invoke(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    final_message = result[\"messages\"][-1]\n",
        "    content = final_message.content\n",
        "\n",
        "    # Split into lines and handle numbering cleanly\n",
        "    if any(content.strip().startswith(f\"{i}.\") for i in range(1, 4)):\n",
        "        lines = content.split(\"\\n\")\n",
        "        for line in lines:\n",
        "            wrapped = textwrap.fill(line, width=80,\n",
        "                                    subsequent_indent=' ' * 4)\n",
        "            print(wrapped)\n",
        "    else:\n",
        "        # Fallback: paragraph wrap\n",
        "        wrapped_output = textwrap.fill(content, width=80)\n",
        "        print(\"Assistant:\\n\", wrapped_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MS7mJobchJk4",
        "outputId": "166f4914-0970-4a2d-f2d2-b13a621ea44f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume Q&A Chatbot (type 'exit' to quit)\n",
            "You: Hi\n",
            "Assistant:\n",
            " Hello! How can I assist you today?\n",
            "You: Tell me about Pavan\n",
            "Assistant:\n",
            " Pavan is a professional with experience as both a Machine Learning Engineer and\n",
            "a Data Scientist. He has worked at PayPal India, where he boosted revenue by\n",
            "onboarding global merchants and preventing fraud losses through ML-based fraud\n",
            "detection models. Pavan is skilled in Python, SQL, Machine Learning, NLP, and\n",
            "Tableau, with a track record of delivering AI/ML solutions that drive measurable\n",
            "business impact. Additionally, he holds a Master of Science in Data Science from\n",
            "the University at Buffalo SUNY and a Bachelor and Master of Technology in\n",
            "Electrical Engineering from the Indian Institute of Technology Madras (IITM).\n",
            "You: What is his Professional Expereience\n",
            "Assistant:\n",
            " Pavan's professional experience includes roles as a Machine Learning Engineer\n",
            "and a Data Scientist at PayPal India. Here are some highlights of his\n",
            "experience:  Machine Learning Engineer (Jan 2021 - Jan 2024): - Boosted revenue\n",
            "by onboarding global merchants and preventing fraud losses through ML-based\n",
            "fraud detection models. - Designed decision tree and anomaly detection\n",
            "techniques to analyze transactions and identify fraudulent patterns. - Built a\n",
            "Siamese CNN model with custom contrastive loss to detect document manipulation.\n",
            "Data Scientist (Jan 2021 - Jan 2024): - Developed predictive models, A/B tests,\n",
            "and causal inference frameworks across fraud detection, recommendation systems,\n",
            "and customer analytics. - Utilized NLP to generate chargeback approval summaries\n",
            "and reduce triage time. - Partnered with product managers to design A/B tests\n",
            "and improve fraud rule accuracy. - Automated real-time KPI dashboards in Tableau\n",
            "for fraud tracking.  These experiences showcase Pavan's expertise in machine\n",
            "learning, data science, and driving business impact through data-driven decision\n",
            "making.\n",
            "You: Did he have any cool projects?\n",
            "Assistant:\n",
            " Yes, Pavan has worked on some interesting and impactful projects during his\n",
            "professional experience. Here are a few notable projects he has been involved\n",
            "in:  1. Built a Siamese CNN Model with Custom Contrastive Loss:    - Developed a\n",
            "Siamese Convolutional Neural Network (CNN) model with custom contrastive loss to\n",
            "detect document manipulation by comparing against true embeddings. This project\n",
            "achieved 94% precision, reducing manual review by 45% and preventing onboarding\n",
            "of high-risk accounts.  2. Fine-Tuned a Domain-Specific LLaMA-2 Model Using NLP\n",
            "Techniques:    - Fine-tuned a domain-specific LLaMA-2 model on 100K+ fraud\n",
            "analyst notes using Natural Language Processing (NLP) techniques. This project\n",
            "aimed to generate soft-approval summaries for flagged chargeback transactions,\n",
            "reducing triage time by 32%.  3. Reproduced Google's TabNet Architecture for\n",
            "Fraud Detection:    - Reproduced Google's TabNet architecture and integrated\n",
            "SHAP/LIME for feature attribution and model explainability on a 250K+\n",
            "transaction dataset. This project improved fraud rejection precision from 55% to\n",
            "67%.  These projects demonstrate Pavan's expertise in machine learning, NLP, and\n",
            "fraud detection, showcasing his ability to develop innovative solutions and\n",
            "drive impactful results in the field of data science and machine learning.\n",
            "You: Did he work on Computer Vision?\n",
            "Assistant:\n",
            " Yes, Pavan has worked on projects involving Computer Vision during his\n",
            "professional experience. One of the projects where he utilized Computer Vision\n",
            "techniques is the development of a Siamese Convolutional Neural Network (CNN)\n",
            "model with custom contrastive loss to detect document manipulation. This project\n",
            "involved comparing documents against true embeddings to identify manipulation,\n",
            "showcasing Pavan's expertise in applying Computer Vision algorithms for fraud\n",
            "detection and document analysis.\n",
            "You: Where did he graduate from?\n",
            "Assistant:\n",
            " Pavan graduated from the Indian Institute of Technology Madras (IITM) in\n",
            "Chennai, India. Additionally, he pursued a Master of Science in Data Science\n",
            "from the University at Buffalo SUNY in Buffalo, NY.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-66-3810905900.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resume Q&A Chatbot (type 'exit' to quit)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import streamlit as st\n",
        "\n",
        "st.title(\"Resume Q&A Chatbot\")\n",
        "chat_history = st.session_state.get(\"chat_history\", [])\n",
        "\n",
        "user_input = st.text_input(\"Ask about Pavan's resume:\")\n",
        "if user_input:\n",
        "    config = {\"configurable\": {\"thread_id\": \"streamlit_thread\"}}\n",
        "    result = graph.invoke(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "        config=config,\n",
        "    )\n",
        "    assistant_message = result[\"messages\"][-1].content\n",
        "    chat_history.append((\"You\", user_input))\n",
        "    chat_history.append((\"Assistant\", assistant_message))\n",
        "    st.session_state.chat_history = chat_history\n",
        "\n",
        "for role, msg in chat_history:\n",
        "    st.markdown(f\"**{role}:** {msg}\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "U60DO_U_hoh7",
        "outputId": "df371f89-8657-40af-f076-3ade16a4e6f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import streamlit as st\\n\\nst.title(\"Resume Q&A Chatbot\")\\nchat_history = st.session_state.get(\"chat_history\", [])\\n\\nuser_input = st.text_input(\"Ask about Pavan\\'s resume:\")\\nif user_input:\\n    config = {\"configurable\": {\"thread_id\": \"streamlit_thread\"}}\\n    result = graph.invoke(\\n        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\\n        config=config,\\n    )\\n    assistant_message = result[\"messages\"][-1].content\\n    chat_history.append((\"You\", user_input))\\n    chat_history.append((\"Assistant\", assistant_message))\\n    st.session_state.chat_history = chat_history\\n\\nfor role, msg in chat_history:\\n    st.markdown(f\"**{role}:** {msg}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# This function takes user input and returns the chatbot's string response.\n",
        "# Gradio automatically handles the UI and history display.\n",
        "def resume_chatbot(user_input, history):\n",
        "    # Use a unique thread_id to keep conversations separate\n",
        "    config = {\"configurable\": {\"thread_id\": \"gradio_thread\"}}\n",
        "\n",
        "    # Run your LangGraph agent\n",
        "    result = graph.invoke(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    # Get the final message content from the agent's response\n",
        "    assistant_message = result[\"messages\"][-1].content\n",
        "\n",
        "    # Return ONLY the assistant's message\n",
        "    return assistant_message\n",
        "\n",
        "# Launch the Gradio interface\n",
        "gr.ChatInterface(fn=resume_chatbot, title=\"Resume Q&A Chatbot\").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "zE_pBVHalE4e",
        "outputId": "0e62618b-1539-4c8f-ec45-841d146a0d91"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6859cbbecc2f973a2d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6859cbbecc2f973a2d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NpmtNFtnnAJO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ICBoFxMBYwER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}